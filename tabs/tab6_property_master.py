# tabs/tab6_property_master.py
from __future__ import annotations

from datetime import datetime
from typing import Optional, Any
from io import BytesIO
import re
import unicodedata

import pandas as pd
import streamlit as st
from firebase_admin import firestore  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®IDä¿å­˜ç”¨


# ==========================
# åˆ—å®šç¾©
# ==========================

# ç‰©ä»¶ãƒã‚¹ã‚¿ï¼ˆç‚¹æ¤œæ¡ä»¶ãƒ»é€£çµ¡æ–¹æ³•ãªã©ï¼‰
MASTER_COLUMNS = [
    "ç®¡ç†ç•ªå·",
    "ç‚¹æ¤œå®Ÿæ–½æœˆ",
    "é€£çµ¡æœŸé™_æ—¥å‰",
    "é€£çµ¡æ–¹æ³•_é›»è©±1",
    "é€£çµ¡æ–¹æ³•_é›»è©±2",
    "é€£çµ¡æ–¹æ³•_FAX1",
    "é€£çµ¡æ–¹æ³•_FAX2",
    "é€£çµ¡æ–¹æ³•_ãƒ¡ãƒ¼ãƒ«1",
    "é€£çµ¡æ–¹æ³•_ãƒ¡ãƒ¼ãƒ«2",
    "é›»è©±ç•ªå·1",
    "é›»è©±ç•ªå·2",
    "FAXç•ªå·1",
    "FAXç•ªå·2",
    "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹1",
    "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹2",
    "é€£çµ¡å®›å1",
    "é€£çµ¡å®›å2",
    "OKæ›œæ—¥",
    "NGæ›œæ—¥",
    "OKæ™‚é–“å¸¯_é–‹å§‹",
    "OKæ™‚é–“å¸¯_çµ‚äº†",
    "NGæ™‚é–“å¸¯_é–‹å§‹",
    "NGæ™‚é–“å¸¯_çµ‚äº†",
    "è²¼ã‚Šç´™ãƒ†ãƒ³ãƒ—ãƒ¬ç¨®åˆ¥",
    "è²¼ã‚Šç´™ãƒ†ãƒ³ãƒ—ãƒ¬_ãƒ‰ãƒ©ã‚¤ãƒ–ID",
    "FAXãƒ†ãƒ³ãƒ—ãƒ¬ç¨®åˆ¥",
    "FAXãƒ†ãƒ³ãƒ—ãƒ¬_ãƒ‰ãƒ©ã‚¤ãƒ–ID",
    "ãƒ¡ãƒ¼ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬_ãƒ‰ãƒ©ã‚¤ãƒ–ID",
    "å‚™è€ƒ",
    "æ›´æ–°æ—¥æ™‚",
    "æœ€çµ‚æ›´æ–°è€…",
]

# ç‰©ä»¶åŸºæœ¬æƒ…å ±ï¼ˆExcel/CSV ã‹ã‚‰å–ã‚Šè¾¼ã‚€ï¼‰
BASIC_COLUMNS = [
    "ç®¡ç†ç•ªå·",
    "ç‰©ä»¶å",
    "ä½æ‰€",
    "çª“å£ä¼šç¤¾",
    "æ‹…å½“éƒ¨ç½²",
    "æ‹…å½“è€…å",
    "å¥‘ç´„ç¨®åˆ¥",
]


# ==========================
# å…±é€šãƒ˜ãƒ«ãƒ‘ãƒ¼
# ==========================

def _normalize_df(df: pd.DataFrame, columns: list[str]) -> pd.DataFrame:
    """æŒ‡å®šåˆ—ã ã‘ã«æƒãˆã¦ã€æ–‡å­—åˆ— + strip ã«çµ±ä¸€"""
    df = df.copy() if df is not None else pd.DataFrame()
    for col in columns:
        if col not in df.columns:
            df[col] = ""
    df = df[columns].copy()
    if not df.empty:
        df = df.astype(str).apply(lambda col: col.str.strip())
    return df


def parse_notice_deadline_to_days(text: str) -> tuple[str, str]:
    """
    ã€Œç‚¹æ¤œé€šçŸ¥å…ˆï¼‘é€šçŸ¥æœŸé™ã€ã®æ–‡å­—åˆ— â†’ æ—¥æ•°ï¼ˆæ–‡å­—åˆ—ï¼‰ã¨ã€è§£æã§ããªã‹ã£ãŸå ´åˆç”¨ã®ãƒ¡ãƒ¢
      - ä¾‹: "1é€±é–“å‰" â†’ ("7", "")
      - ä¾‹: "10æ—¥å‰" â†’ ("10", "")
      - ãã‚Œä»¥å¤– â†’ ("", å…ƒã®æ–‡å­—åˆ—)
    """
    s = str(text or "").strip()
    if not s:
        return "", ""

    s_norm = unicodedata.normalize("NFKC", s)  # å…¨è§’â†’åŠè§’ãªã©
    # ã€‡é€±é–“
    m = re.search(r"(\d+)\s*é€±", s_norm)
    if m:
        days = int(m.group(1)) * 7
        return str(days), ""
    # ã€‡æ—¥å‰ / ã€‡æ—¥
    m = re.search(r"(\d+)\s*æ—¥", s_norm)
    if m:
        days = int(m.group(1))
        return str(days), ""

    # è§£æã§ããªã„ã‚‚ã®ã¯å‚™è€ƒå´ã¸å›ã™
    return "", s


# ==========================
# Sheets ãƒ˜ãƒ«ãƒ‘ãƒ¼
# ==========================

def ensure_sheet_and_headers(
    sheets_service: Any,
    spreadsheet_id: str,
    sheet_title: str,
    headers: list[str],
) -> None:
    """
    æŒ‡å®šã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆå†…ã«ã‚·ãƒ¼ãƒˆã‚’ä½œæˆã—ã€
    1è¡Œç›®ã«ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚»ãƒƒãƒˆã™ã‚‹ï¼ˆãªã‘ã‚Œã°ï¼‰ã€‚
    """
    if not sheets_service or not spreadsheet_id:
        return

    # ã‚·ãƒ¼ãƒˆä¸€è¦§å–å¾—
    meta = sheets_service.spreadsheets().get(spreadsheetId=spreadsheet_id).execute()
    sheets = meta.get("sheets", [])
    existing_titles = {s["properties"]["title"] for s in sheets}

    # ã‚·ãƒ¼ãƒˆãŒãªã‘ã‚Œã°è¿½åŠ 
    if sheet_title not in existing_titles:
        body = {
            "requests": [
                {
                    "addSheet": {
                        "properties": {
                            "title": sheet_title,
                        }
                    }
                }
            ]
        }
        sheets_service.spreadsheets().batchUpdate(
            spreadsheetId=spreadsheet_id,
            body=body,
        ).execute()

    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®ç¢ºèª
    range_header = f"{sheet_title}!1:1"
    result = sheets_service.spreadsheets().values().get(
        spreadsheetId=spreadsheet_id,
        range=range_header,
    ).execute()
    values = result.get("values", [])

    need_update_header = False
    if not values:
        need_update_header = True
    else:
        current_header = values[0]
        if current_header != headers:
            need_update_header = True

    if need_update_header:
        body = {"values": [headers]}
        sheets_service.spreadsheets().values().update(
            spreadsheetId=spreadsheet_id,
            range=f"{sheet_title}!A1",
            valueInputOption="RAW",
            body=body,
        ).execute()


def create_property_master_spreadsheet(
    sheets_service: Any,
    user_email: Optional[str] = None,
) -> str:
    """
    ç‰©ä»¶åŸºæœ¬æƒ…å ± / ç‰©ä»¶ãƒã‚¹ã‚¿ ã®2ã‚·ãƒ¼ãƒˆã‚’æŒã¤ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’æ–°è¦ä½œæˆã—ã€
    ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®šã—ã¦ Spreadsheet ID ã‚’è¿”ã™ã€‚
    """
    if not sheets_service:
        raise RuntimeError("Sheets service is not initialized")

    title_suffix = user_email or "property_master"
    body = {
        "properties": {
            "title": f"ç‰©ä»¶ãƒã‚¹ã‚¿_{title_suffix}",
        },
        "sheets": [
            {"properties": {"title": "ç‰©ä»¶åŸºæœ¬æƒ…å ±"}},
            {"properties": {"title": "ç‰©ä»¶ãƒã‚¹ã‚¿"}},
        ],
    }
    resp = sheets_service.spreadsheets().create(body=body).execute()
    spreadsheet_id = resp["spreadsheetId"]

    # ãƒ˜ãƒƒãƒ€ãƒ¼æ›¸ãè¾¼ã¿
    ensure_sheet_and_headers(sheets_service, spreadsheet_id, "ç‰©ä»¶åŸºæœ¬æƒ…å ±", BASIC_COLUMNS)
    ensure_sheet_and_headers(sheets_service, spreadsheet_id, "ç‰©ä»¶ãƒã‚¹ã‚¿", MASTER_COLUMNS)

    return spreadsheet_id


def load_sheet_as_df(
    sheets_service: Any,
    spreadsheet_id: str,
    sheet_title: str,
    columns: list[str],
) -> pd.DataFrame:
    """
    A1 ã‹ã‚‰ã®å†…å®¹ã‚’ DataFrame ã¨ã—ã¦å–å¾—ã—ã€æŒ‡å®šåˆ—ã«æƒãˆã¦è¿”ã™ã€‚
    â€» è¡Œã«ã‚ˆã£ã¦åˆ—æ•°ãŒãƒãƒ©ãƒãƒ©ã§ã‚‚ã€ãƒ˜ãƒƒãƒ€ãƒ¼æ•°ã«åˆã‚ã›ã¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ã€‚
    """
    if not sheets_service or not spreadsheet_id:
        return pd.DataFrame(columns=columns)

    range_name = f"{sheet_title}!A1:ZZ"
    try:
        result = sheets_service.spreadsheets().values().get(
            spreadsheetId=spreadsheet_id,
            range=range_name,
        ).execute()
    except Exception as e:
        st.error(f"{sheet_title} ã‚·ãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return pd.DataFrame(columns=columns)

    values = result.get("values", [])
    if not values:
        return pd.DataFrame(columns=columns)

    header = values[0]
    rows = values[1:] if len(values) > 1 else []

    # è¡Œã”ã¨ã®å·®ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼åˆ‡ã‚Šè©°ã‚
    padded_rows = []
    for row in rows:
        if len(row) < len(header):
            padded_rows.append(row + [""] * (len(header) - len(row)))
        elif len(row) > len(header):
            padded_rows.append(row[:len(header)])
        else:
            padded_rows.append(row)

    df = pd.DataFrame(padded_rows, columns=header)
    df = df.astype(str).apply(lambda col: col.str.strip())

    # è¶³ã‚Šãªã„åˆ—è£œå®Œ
    for col in columns:
        if col not in df.columns:
            df[col] = ""
    return df[columns].copy()


def save_df_to_sheet(
    sheets_service: Any,
    spreadsheet_id: str,
    sheet_title: str,
    df: pd.DataFrame,
    columns: list[str],
) -> None:
    """æŒ‡å®š DataFrame ã‚’ãƒ˜ãƒƒãƒ€ãƒ¼è¾¼ã¿ã§ã‚·ãƒ¼ãƒˆã«ã¾ã‚‹ã”ã¨æ›¸ãæˆ»ã™ã€‚"""
    if not sheets_service or not spreadsheet_id:
        return

    df_to_save = _normalize_df(df, columns)
    values = [columns] + df_to_save.values.tolist()

    try:
        # ã‚·ãƒ¼ãƒˆå…¨ä½“ã‚¯ãƒªã‚¢
        sheets_service.spreadsheets().values().clear(
            spreadsheetId=spreadsheet_id,
            range=sheet_title,
        ).execute()

        body = {"values": values}
        sheets_service.spreadsheets().values().update(
            spreadsheetId=spreadsheet_id,
            range=f"{sheet_title}!A1",
            valueInputOption="RAW",
            body=body,
        ).execute()
    except Exception as e:
        st.error(f"{sheet_title} ã‚·ãƒ¼ãƒˆã¸ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        raise


# ==========================
# ç‰©ä»¶åŸºæœ¬æƒ…å ±ï¼šExcel/CSV èª­ã¿è¾¼ã¿ & å·®åˆ†
# ==========================

def load_raw_from_uploaded(uploaded_file) -> pd.DataFrame:
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸ Excel/CSV ã‚’ã€Œãã®ã¾ã¾ã€DataFrame ã«ã™ã‚‹ï¼ˆå…¨åˆ—ä¿æŒï¼‰ã€‚
    æ–‡å­—åˆ—åŒ–ï¼†stripã ã‘å®Ÿæ–½ã€‚
    """
    if uploaded_file is None:
        return pd.DataFrame()

    name = uploaded_file.name.lower()

    # Excel
    if name.endswith(".xlsx") or name.endswith(".xls"):
        df = pd.read_excel(uploaded_file, dtype=str)
        if not df.empty:
            df = df.astype(str).apply(lambda col: col.str.strip())
        return df

    # CSV
    raw_bytes = uploaded_file.read()
    encodings_to_try = ["utf-8", "utf-8-sig", "cp932"]
    last_err: Optional[Exception] = None

    for enc in encodings_to_try:
        try:
            df = pd.read_csv(BytesIO(raw_bytes), dtype=str, encoding=enc)
            if not df.empty:
                df = df.astype(str).apply(lambda col: col.str.strip())
            return df
        except UnicodeDecodeError as e:
            last_err = e
            continue
        except Exception as e:
            last_err = e
            continue

    st.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ï¼ˆæœ€å¾Œã®ã‚¨ãƒ©ãƒ¼: {last_err}ï¼‰")
    return pd.DataFrame()


def _map_basic_from_raw_df(df: pd.DataFrame) -> pd.DataFrame:
    """
    å…ƒã® DataFrameï¼ˆã©ã‚“ãªãƒ˜ãƒƒãƒ€ãƒ¼åã§ã‚‚OKï¼‰ã‹ã‚‰ BASIC_COLUMNS ã‚’æ§‹æˆã™ã‚‹ã€‚
    ä¾‹:
      - ç®¡ç†ç•ªå·  â† ç‰©ä»¶ã®ç®¡ç†ç•ªå· / ç‰©ä»¶ç®¡ç†ç•ªå· / ç®¡ç†ç•ªå·
      - ä½æ‰€     â† ç‰©ä»¶æƒ…å ±-ä½æ‰€1 / ä½æ‰€ / æ‰€åœ¨åœ°
      - å¥‘ç´„ç¨®åˆ¥ â† å¥‘ç´„ç¨®é¡ / å¥‘ç´„ç¨®åˆ¥
      - çª“å£ä¼šç¤¾ â† çª“å£åå„ªå…ˆã€ãªã‘ã‚Œã°å¥‘ç´„å…ˆåã€ãªã‘ã‚Œã°çª“å£ä¼šç¤¾
    """
    df = df.copy()
    if not df.empty:
        df = df.astype(str).apply(lambda col: col.str.strip())
    n = len(df)

    def pick(*names: str) -> pd.Series:
        for name in names:
            if name in df.columns:
                return df[name]
        return pd.Series([""] * n)

    mapped = pd.DataFrame()
    mapped["ç®¡ç†ç•ªå·"] = pick("ç®¡ç†ç•ªå·", "ç‰©ä»¶ã®ç®¡ç†ç•ªå·", "ç‰©ä»¶ç®¡ç†ç•ªå·", "ç‰©ä»¶ç•ªå·")
    mapped["ç‰©ä»¶å"] = pick("ç‰©ä»¶å", "æ–½è¨­å")
    mapped["ä½æ‰€"] = pick("ä½æ‰€", "ç‰©ä»¶æƒ…å ±-ä½æ‰€1", "ä½æ‰€1", "æ‰€åœ¨åœ°")
    # çª“å£ä¼šç¤¾ï¼šçª“å£å â†’ å¥‘ç´„å…ˆå â†’ çª“å£ä¼šç¤¾ ã®é †ã§å„ªå…ˆ
    mapped["çª“å£ä¼šç¤¾"] = pick("çª“å£å", "å¥‘ç´„å…ˆå", "çª“å£ä¼šç¤¾")
    mapped["æ‹…å½“éƒ¨ç½²"] = pick("æ‹…å½“éƒ¨ç½²", "éƒ¨ç½²å")
    mapped["æ‹…å½“è€…å"] = pick("æ‹…å½“è€…å", "æ‹…å½“è€…")
    mapped["å¥‘ç´„ç¨®åˆ¥"] = pick("å¥‘ç´„ç¨®åˆ¥", "å¥‘ç´„ç¨®é¡")

    # ç®¡ç†ç•ªå·ãŒç©ºã®è¡Œã¯é™¤å¤–
    mapped = mapped[mapped["ç®¡ç†ç•ªå·"].astype(str).str.strip() != ""].reset_index(drop=True)

    return _normalize_df(mapped, BASIC_COLUMNS)


def diff_basic_info(current_df: pd.DataFrame, new_df: pd.DataFrame):
    """
    current_df: ç¾åœ¨ã‚·ãƒ¼ãƒˆã«å…¥ã£ã¦ã„ã‚‹åŸºæœ¬æƒ…å ±
    new_df    : æ–°ã—ãã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸåŸºæœ¬æƒ…å ±ï¼ˆBASIC_COLUMNSï¼‰

    æˆ»ã‚Šå€¤:
      - new_rows     : æ–°è¦è¿½åŠ è¡Œ
      - updated_rows : æ›´æ–°è¡Œï¼ˆæ–°ã—ã„å€¤ã€‚æ—§å€¤ã¯ *_æ—§ åˆ—ã§æŒã¤ï¼‰
      - deleted_rows : å‰Šé™¤å€™è£œè¡Œ
    """
    cur = _normalize_df(current_df, BASIC_COLUMNS)
    new = _normalize_df(new_df, BASIC_COLUMNS)

    cur_ids = set(cur["ç®¡ç†ç•ªå·"])
    new_ids = set(new["ç®¡ç†ç•ªå·"])

    new_only_ids = new_ids - cur_ids
    deleted_ids = cur_ids - new_ids
    common_ids = cur_ids & new_ids

    new_rows = new[new["ç®¡ç†ç•ªå·"].isin(new_only_ids)].copy()
    deleted_rows = cur[cur["ç®¡ç†ç•ªå·"].isin(deleted_ids)].copy()

    cur_common = cur[cur["ç®¡ç†ç•ªå·"].isin(common_ids)].set_index("ç®¡ç†ç•ªå·")
    new_common = new[new["ç®¡ç†ç•ªå·"].isin(common_ids)].set_index("ç®¡ç†ç•ªå·")

    changed_ids = []
    for mid in common_ids:
        if not cur_common.loc[mid].equals(new_common.loc[mid]):
            changed_ids.append(mid)

    updated_cur = cur_common.loc[changed_ids].reset_index()
    updated_new = new_common.loc[changed_ids].reset_index()

    updated_rows = updated_new.copy()
    for col in BASIC_COLUMNS:
        if col == "ç®¡ç†ç•ªå·":
            continue
        updated_rows[f"{col}_æ—§"] = updated_cur[col].values

    return new_rows, updated_rows, deleted_rows


# ==========================
# ç‰©ä»¶ãƒã‚¹ã‚¿ã¸ã®è‡ªå‹•ãƒãƒƒãƒ”ãƒ³ã‚°
# ==========================

def _map_master_from_raw_df(df: pd.DataFrame) -> pd.DataFrame:
    """
    å…ƒã® DataFrame ã‹ã‚‰ç‰©ä»¶ãƒã‚¹ã‚¿ç”¨ã® MASTER_COLUMNS ã‚’æ§‹æˆã™ã‚‹ã€‚
    ï¼ˆç®¡ç†ç•ªå·ãŒç©ºã®è¡Œã¯é™¤å¤–ï¼‰
    """
    df = df.copy()
    if not df.empty:
        df = df.astype(str).apply(lambda col: col.str.strip())
    if df.empty:
        return pd.DataFrame(columns=MASTER_COLUMNS)

    n_all = len(df)

    def pick0(*names: str) -> pd.Series:
        for name in names:
            if name in df.columns:
                return df[name]
        return pd.Series([""] * n_all)

    # ã¾ãšç®¡ç†ç•ªå·ã‚’è¦‹ã¦ã€ç©ºè¡Œã‚’é™¤å¤–
    mgmt_all = pick0("ç®¡ç†ç•ªå·", "ç‰©ä»¶ã®ç®¡ç†ç•ªå·", "ç‰©ä»¶ç®¡ç†ç•ªå·", "ç‰©ä»¶ç•ªå·")
    mask = mgmt_all.astype(str).str.strip() != ""
    df2 = df[mask].reset_index(drop=True)
    if df2.empty:
        return pd.DataFrame(columns=MASTER_COLUMNS)

    n = len(df2)

    def pick(*names: str) -> pd.Series:
        for name in names:
            if name in df2.columns:
                return df2[name]
        return pd.Series([""] * n)

    # å‡ºåŠ›ç”¨ DataFrameï¼ˆå…¨åˆ—ç©ºã§åˆæœŸåŒ–ï¼‰
    out = pd.DataFrame({col: [""] * n for col in MASTER_COLUMNS})

    # ç®¡ç†ç•ªå·
    out["ç®¡ç†ç•ªå·"] = pick("ç®¡ç†ç•ªå·", "ç‰©ä»¶ã®ç®¡ç†ç•ªå·", "ç‰©ä»¶ç®¡ç†ç•ªå·", "ç‰©ä»¶ç•ªå·")

    # ç‚¹æ¤œå®Ÿæ–½æœˆ â† ç‚¹æ¤œæœˆãã®ã¾ã¾
    out["ç‚¹æ¤œå®Ÿæ–½æœˆ"] = pick("ç‚¹æ¤œæœˆ", "ç‚¹æ¤œå®Ÿæ–½æœˆ")

    # é€£çµ¡æœŸé™_æ—¥å‰ ï¼‹ é€šçŸ¥æœŸé™ã®åŸæ–‡ã‚’å‚™è€ƒç”¨ã«ä¿æŒ
    deadline_series = pick("ç‚¹æ¤œé€šçŸ¥å…ˆï¼‘é€šçŸ¥æœŸé™", "ç‚¹æ¤œé€šçŸ¥å…ˆ1é€šçŸ¥æœŸé™")
    days_list: list[str] = []
    notes_from_deadline: list[str] = []
    for v in deadline_series:
        days, note = parse_notice_deadline_to_days(v)
        days_list.append(days)
        notes_from_deadline.append(note)
    out["é€£çµ¡æœŸé™_æ—¥å‰"] = pd.Series(days_list)

    # é€šçŸ¥æ–¹æ³•
    method1 = pick("ç‚¹æ¤œé€šçŸ¥å…ˆï¼‘é€šçŸ¥æ–¹æ³•", "ç‚¹æ¤œé€šçŸ¥å…ˆ1é€šçŸ¥æ–¹æ³•")
    method2 = pick("ç‚¹æ¤œé€šçŸ¥å…ˆï¼’é€šçŸ¥æ–¹æ³•", "ç‚¹æ¤œé€šçŸ¥å…ˆ2é€šçŸ¥æ–¹æ³•")

    tel1_series = pick("ç‚¹æ¤œé€šçŸ¥å…ˆï¼‘TEL", "ç‚¹æ¤œé€šçŸ¥å…ˆ1TEL")
    tel2_series = pick("ç‚¹æ¤œé€šçŸ¥å…ˆï¼’TEL", "ç‚¹æ¤œé€šçŸ¥å…ˆ2TEL")
    tel_fallback = pick("TEL")

    fax1_series = pick("ç‚¹æ¤œé€šçŸ¥å…ˆï¼‘FAX", "ç‚¹æ¤œé€šçŸ¥å…ˆ1FAX")
    fax2_series = pick("ç‚¹æ¤œé€šçŸ¥å…ˆï¼’FAX", "ç‚¹æ¤œé€šçŸ¥å…ˆ2FAX")
    fax_fallback = pick("FAX")

    mail1_series = pick("ç‚¹æ¤œé€šçŸ¥å…ˆï¼‘Email/URL", "ç‚¹æ¤œé€šçŸ¥å…ˆ1Email/URL", "ç‚¹æ¤œé€šçŸ¥å…ˆï¼‘Email", "ç‚¹æ¤œé€šçŸ¥å…ˆ1Email")
    mail2_series = pick("ç‚¹æ¤œé€šçŸ¥å…ˆï¼’Email/URL", "ç‚¹æ¤œé€šçŸ¥å…ˆ2Email/URL", "ç‚¹æ¤œé€šçŸ¥å…ˆï¼’Email", "ç‚¹æ¤œé€šçŸ¥å…ˆ2Email")

    window_name = pick("çª“å£å")
    contract_name = pick("å¥‘ç´„å…ˆå")
    contact2_name = pick("ç‚¹æ¤œé€šçŸ¥å…ˆï¼’ç‚¹æ¤œé€šçŸ¥å…ˆ", "ç‚¹æ¤œé€šçŸ¥å…ˆ2ç‚¹æ¤œé€šçŸ¥å…ˆ")

    sticker_type = pick("è²¼ç´™è²¼ä»˜æ›¸å¼", "è²¼ç´™è²¼ä»˜æ§˜å¼")
    sticker_count = pick("è²¼ç´™æšæ•°")

    notes_combined: list[str] = []

    for i in range(n):
        # --- é€£çµ¡æ–¹æ³•1 ---
        m1_raw = str(method1.iloc[i]) if i < len(method1) else ""
        m1_norm = unicodedata.normalize("NFKC", m1_raw).upper()
        if ("TEL" in m1_norm) or ("é›»è©±" in m1_raw):
            out.at[i, "é€£çµ¡æ–¹æ³•_é›»è©±1"] = "1"
        if ("FAX" in m1_norm) or ("ï¼¦ï¼¡ï¼¸" in m1_raw):
            out.at[i, "é€£çµ¡æ–¹æ³•_FAX1"] = "1"
        if ("MAIL" in m1_norm) or ("ï¾’ï½°ï¾™" in m1_raw) or ("ãƒ¡ãƒ¼ãƒ«" in m1_raw):
            out.at[i, "é€£çµ¡æ–¹æ³•_ãƒ¡ãƒ¼ãƒ«1"] = "1"

        # --- é€£çµ¡æ–¹æ³•2 ---
        m2_raw = str(method2.iloc[i]) if i < len(method2) else ""
        m2_norm = unicodedata.normalize("NFKC", m2_raw).upper()
        if ("TEL" in m2_norm) or ("é›»è©±" in m2_raw):
            out.at[i, "é€£çµ¡æ–¹æ³•_é›»è©±2"] = "2"
        if ("FAX" in m2_norm) or ("ï¼¦ï¼¡ï¼¸" in m2_raw):
            out.at[i, "é€£çµ¡æ–¹æ³•_FAX2"] = "2"
        if ("MAIL" in m2_norm) or ("ï¾’ï½°ï¾™" in m2_raw) or ("ãƒ¡ãƒ¼ãƒ«" in m2_raw):
            out.at[i, "é€£çµ¡æ–¹æ³•_ãƒ¡ãƒ¼ãƒ«2"] = "2"

        # --- é›»è©±ç•ªå· ---
        tel1 = str(tel1_series.iloc[i]) if i < len(tel1_series) else ""
        tel_fb = str(tel_fallback.iloc[i]) if i < len(tel_fallback) else ""
        tel2 = str(tel2_series.iloc[i]) if i < len(tel2_series) else ""
        out.at[i, "é›»è©±ç•ªå·1"] = tel1 or tel_fb
        out.at[i, "é›»è©±ç•ªå·2"] = tel2

        # --- FAXç•ªå· ---
        fax1 = str(fax1_series.iloc[i]) if i < len(fax1_series) else ""
        fax_fb = str(fax_fallback.iloc[i]) if i < len(fax_fallback) else ""
        fax2 = str(fax2_series.iloc[i]) if i < len(fax2_series) else ""
        out.at[i, "FAXç•ªå·1"] = fax1 or fax_fb
        out.at[i, "FAXç•ªå·2"] = fax2

        # --- ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ ---
        mail1 = str(mail1_series.iloc[i]) if i < len(mail1_series) else ""
        mail2 = str(mail2_series.iloc[i]) if i < len(mail2_series) else ""
        out.at[i, "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹1"] = mail1
        out.at[i, "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹2"] = mail2

        # --- é€£çµ¡å®›å ---
        win = str(window_name.iloc[i]) if i < len(window_name) else ""
        con = str(contract_name.iloc[i]) if i < len(contract_name) else ""
        out.at[i, "é€£çµ¡å®›å1"] = win or con

        cn2 = str(contact2_name.iloc[i]) if i < len(contact2_name) else ""
        out.at[i, "é€£çµ¡å®›å2"] = cn2

        # --- è²¼ã‚Šç´™ãƒ†ãƒ³ãƒ—ãƒ¬ç¨®åˆ¥ ---
        stype = str(sticker_type.iloc[i]) if i < len(sticker_type) else ""
        out.at[i, "è²¼ã‚Šç´™ãƒ†ãƒ³ãƒ—ãƒ¬ç¨®åˆ¥"] = stype

        # --- å‚™è€ƒ ---
        note_parts = []
        if notes_from_deadline[i]:
            note_parts.append(f"é€šçŸ¥æœŸé™: {notes_from_deadline[i]}")
        sc = str(sticker_count.iloc[i]) if i < len(sticker_count) else ""
        if sc:
            note_parts.append(f"è²¼ç´™æšæ•°: {sc}")
        notes_combined.append(" / ".join([p for p in note_parts if p]))

    if "å‚™è€ƒ" in out.columns:
        out["å‚™è€ƒ"] = notes_combined

    return _normalize_df(out, MASTER_COLUMNS)


# ==========================
# ãƒãƒ¼ã‚¸å‡¦ç†
# ==========================

def merge_master_and_basic(master_df: pd.DataFrame, basic_df: pd.DataFrame) -> pd.DataFrame:
    """ç®¡ç†ç•ªå·ã§ç‰©ä»¶ãƒã‚¹ã‚¿ã¨åŸºæœ¬æƒ…å ±ã‚’ãƒãƒ¼ã‚¸ã—ã¦è¡¨ç¤ºç”¨ DataFrame ã«ã™ã‚‹ã€‚"""
    master_df = _normalize_df(master_df, MASTER_COLUMNS)
    basic_df = _normalize_df(basic_df, BASIC_COLUMNS)

    if master_df.empty:
        merged = basic_df.copy()
        for col in MASTER_COLUMNS:
            if col not in merged.columns:
                merged[col] = ""
        return merged

    merged = master_df.merge(
        basic_df,
        on="ç®¡ç†ç•ªå·",
        how="left",
        suffixes=("", "_åŸºæœ¬"),
    )

    display_cols = (
        ["ç®¡ç†ç•ªå·", "ç‰©ä»¶å", "ä½æ‰€", "çª“å£ä¼šç¤¾", "æ‹…å½“éƒ¨ç½²", "æ‹…å½“è€…å", "å¥‘ç´„ç¨®åˆ¥"]
        + [col for col in MASTER_COLUMNS if col != "ç®¡ç†ç•ªå·"]
    )
    display_cols = [c for c in display_cols if c in merged.columns]
    return merged[display_cols]


# ==========================
# ãƒ¡ã‚¤ãƒ³ UI
# ==========================

def render_tab6_property_master(
    sheets_service: Any,
    default_spreadsheet_id: str = "",
    basic_sheet_title: str = "ç‰©ä»¶åŸºæœ¬æƒ…å ±",
    master_sheet_title: str = "ç‰©ä»¶ãƒã‚¹ã‚¿",
    current_user_email: Optional[str] = None,
):
    """
    ç‰©ä»¶ãƒã‚¹ã‚¿ç®¡ç†ã‚¿ãƒ–
    - ç‰©ä»¶åŸºæœ¬æƒ…å ± / ç‰©ä»¶ãƒã‚¹ã‚¿ ã‚’åŒä¸€ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®åˆ¥ã‚·ãƒ¼ãƒˆã¨ã—ã¦ç®¡ç†
    - Excel/CSV ã‹ã‚‰åŸºæœ¬æƒ…å ±ã‚’å–ã‚Šè¾¼ã¿ã€å·®åˆ†ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ â†’ ã‚·ãƒ¼ãƒˆåæ˜ 
    - ç‰©ä»¶ãƒã‚¹ã‚¿ã¯ã€Œã‚¤ãƒ³ãƒãƒ¼ãƒˆæ™‚ã«æ–°è¦ç®¡ç†ç•ªå·ã ã‘è‡ªå‹•åˆæœŸåŒ–ã€
    - ç‰©ä»¶ãƒã‚¹ã‚¿ç”¨ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã« Firestore ã«ä¿å­˜
    """
    st.subheader("ç‰©ä»¶ãƒã‚¹ã‚¿ç®¡ç†")

    # ------------------------------
    # Firestore ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®IDã‚’èª­ã¿è¾¼ã¿
    # ------------------------------
    db = None
    stored_sheet_id: Optional[str] = None
    if current_user_email:
        try:
            db = firestore.client()
            doc = db.collection("user_settings").document(current_user_email).get()
            if doc.exists:
                stored_sheet_id = (doc.to_dict() or {}).get("property_master_spreadsheet_id") or None
        except Exception as e:
            st.warning(f"ç‰©ä»¶ãƒã‚¹ã‚¿ç”¨ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    # åˆå›ãƒ­ãƒ¼ãƒ‰æ™‚ï¼šsession_state ã«ã¾ã å…¥ã£ã¦ãªã‘ã‚Œã° Firestore or default ã‹ã‚‰ã‚»ãƒƒãƒˆ
    if "pm_spreadsheet_id" not in st.session_state or not st.session_state["pm_spreadsheet_id"]:
        initial_id = stored_sheet_id or default_spreadsheet_id
        if initial_id:
            st.session_state["pm_spreadsheet_id"] = initial_id

    # ------------------------------
    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè¨­å®š & æ–°è¦ä½œæˆ
    # ------------------------------
    with st.expander("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè¨­å®š", expanded=True):
        col1, col2 = st.columns([3, 2])

        # 1) å…ˆã«ã€Œæ–°è¦ä½œæˆãƒœã‚¿ãƒ³ã€ã‚’å‡¦ç†ã—ã€å¿…è¦ãªã‚‰ session_state ã« ID ã‚’ã‚»ãƒƒãƒˆ
        with col2:
            st.write("ã€€")
            if st.button("ğŸ†• æ–°è¦ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä½œæˆ", use_container_width=True):
                if not sheets_service:
                    st.error("Sheets API ã®ã‚µãƒ¼ãƒ“ã‚¹ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                else:
                    try:
                        new_id = create_property_master_spreadsheet(
                            sheets_service,
                            user_email=current_user_email,
                        )
                        st.session_state["pm_spreadsheet_id"] = new_id
                        st.success(f"æ–°ã—ã„ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸã€‚\nID: {new_id}")

                        # Firestore ã«ä¿å­˜ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ï¼‰
                        if db and current_user_email:
                            try:
                                db.collection("user_settings").document(current_user_email).set(
                                    {"property_master_spreadsheet_id": new_id},
                                    merge=True,
                                )
                                st.info("ã“ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã«ä¿å­˜ã—ã¾ã—ãŸã€‚æ¬¡å›ã‹ã‚‰è‡ªå‹•ã§èª­ã¿è¾¼ã¾ã‚Œã¾ã™ã€‚")
                            except Exception as ee:
                                st.warning(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {ee}")

                    except Exception as e:
                        st.error(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®æ–°è¦ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

        # 2) session_state ã«å…¥ã£ã¦ã„ã‚‹å€¤ or default ã‹ã‚‰ text_input ã‚’è¡¨ç¤º
        default_id = st.session_state.get("pm_spreadsheet_id", default_spreadsheet_id)
        with col1:
            spreadsheet_id = st.text_input(
                "ç‰©ä»¶ãƒã‚¹ã‚¿ç”¨ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆID",
                value=default_id,
                key="pm_spreadsheet_id",
                help="ç‰©ä»¶åŸºæœ¬æƒ…å ± / ç‰©ä»¶ãƒã‚¹ã‚¿ ã‚’ä¿å­˜ã™ã‚‹ Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã® ID ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
            )

        col3, col4 = st.columns(2)
        with col3:
            basic_title = st.text_input(
                "ç‰©ä»¶åŸºæœ¬æƒ…å ±ã‚·ãƒ¼ãƒˆå",
                value=st.session_state.get("pm_basic_sheet_title", basic_sheet_title),
                key="pm_basic_sheet_title",
            )
        with col4:
            master_title = st.text_input(
                "ç‰©ä»¶ãƒã‚¹ã‚¿ã‚·ãƒ¼ãƒˆå",
                value=st.session_state.get("pm_master_sheet_title", master_sheet_title),
                key="pm_master_sheet_title",
            )

        load_btn = st.button("ç‰©ä»¶ãƒã‚¹ã‚¿ ï¼‹ åŸºæœ¬æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€", type="primary")

        # æ‰‹å…¥åŠ›ã§IDã‚’å¤‰æ›´ã—ãŸå ´åˆã‚‚ Firestore ã«ä¿å­˜
        if db and current_user_email and spreadsheet_id:
            try:
                if stored_sheet_id != spreadsheet_id:
                    db.collection("user_settings").document(current_user_email).set(
                        {"property_master_spreadsheet_id": spreadsheet_id},
                        merge=True,
                    )
            except Exception as e:
                st.warning(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    # ------------------------------
    # ç‰©ä»¶åŸºæœ¬æƒ…å ±ï¼šExcel/CSV â†’ ã‚·ãƒ¼ãƒˆ
    # ------------------------------
    with st.expander("ç‰©ä»¶åŸºæœ¬æƒ…å ±ï¼ˆExcel/CSV ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰", expanded=False):
        st.caption("â€» åŸæœ¬ã¨ãªã‚‹ Excel/CSV ã‹ã‚‰ã€ç‰©ä»¶åŸºæœ¬æƒ…å ±ã€ã‚·ãƒ¼ãƒˆã‚’æ›´æ–°ã—ã¾ã™ã€‚é€šå¸¸ã¯æœ€åˆã«1å›è¡Œã„ã€å¤‰æ›´ãŒã‚ã£ãŸã¨ãã®ã¿å†å®Ÿè¡Œã—ã¾ã™ã€‚")

        uploaded_basic = st.file_uploader(
            "ç‰©ä»¶åŸºæœ¬æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆExcel or CSVï¼‰",
            type=["xlsx", "xls", "csv"],
            key="pm_basic_file_upload",
        )

        col_u1, col_u2 = st.columns(2)
        with col_u1:
            preview_diff_btn = st.button("å·®åˆ†ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", key="pm_preview_diff")
        with col_u2:
            apply_diff_btn = st.button("å·®åˆ†ã‚’ã‚·ãƒ¼ãƒˆã«åæ˜ ", key="pm_apply_diff")

        # å·®åˆ†ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        if preview_diff_btn:
            if not spreadsheet_id:
                st.error("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã‚’å…ˆã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
            elif not sheets_service:
                st.error("Sheets API ã®ã‚µãƒ¼ãƒ“ã‚¹ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            elif uploaded_basic is None:
                st.error("Excel/CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            else:
                try:
                    # ã‚·ãƒ¼ãƒˆã¨ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’äº‹å‰ã«æº–å‚™
                    ensure_sheet_and_headers(
                        sheets_service,
                        spreadsheet_id,
                        basic_title,
                        BASIC_COLUMNS,
                    )
                    current_df = load_sheet_as_df(
                        sheets_service,
                        spreadsheet_id,
                        basic_title,
                        BASIC_COLUMNS,
                    )

                    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ« â†’ ç”Ÿã®DF â†’ åŸºæœ¬æƒ…å ±DFã«ãƒãƒƒãƒ”ãƒ³ã‚°
                    raw_df = load_raw_from_uploaded(uploaded_basic)
                    new_df = _map_basic_from_raw_df(raw_df)

                    new_rows, updated_rows, deleted_rows = diff_basic_info(current_df, new_df)

                    st.session_state["pm_basic_uploaded_raw_df"] = raw_df
                    st.session_state["pm_basic_uploaded_df"] = new_df
                    st.session_state["pm_basic_new_rows"] = new_rows
                    st.session_state["pm_basic_updated_rows"] = updated_rows
                    st.session_state["pm_basic_deleted_rows"] = deleted_rows

                    st.success("å·®åˆ†ã‚’è¨ˆç®—ã—ã¾ã—ãŸã€‚")
                except Exception as e:
                    st.error(f"å·®åˆ†è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

        # å·®åˆ†è¡¨ç¤º
        new_rows = st.session_state.get("pm_basic_new_rows")
        updated_rows = st.session_state.get("pm_basic_updated_rows")
        deleted_rows = st.session_state.get("pm_basic_deleted_rows")

        if isinstance(new_rows, pd.DataFrame):
            st.write(f"âœ… æ–°è¦è¿½åŠ å€™è£œ: {len(new_rows)} ä»¶")
            if len(new_rows) > 0:
                st.dataframe(new_rows, use_container_width=True, height=200)

        if isinstance(updated_rows, pd.DataFrame):
            st.write(f"âœ… æ›´æ–°å€™è£œ: {len(updated_rows)} ä»¶")
            if len(updated_rows) > 0:
                st.dataframe(updated_rows, use_container_width=True, height=200)

        if isinstance(deleted_rows, pd.DataFrame):
            st.write(f"âš ï¸ å‰Šé™¤å€™è£œ: {len(deleted_rows)} ä»¶ï¼ˆâ€»åæ˜ æ™‚ã¯æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã§ã‚·ãƒ¼ãƒˆå…¨ä½“ã‚’ç½®ãæ›ãˆã¾ã™ï¼‰")
            if len(deleted_rows) > 0:
                st.dataframe(deleted_rows, use_container_width=True, height=200)

        # å·®åˆ†åæ˜ ï¼ˆåŸºæœ¬æƒ…å ±ï¼‹ç‰©ä»¶ãƒã‚¹ã‚¿è‡ªå‹•åˆæœŸç™»éŒ²ï¼‰
        if apply_diff_btn:
            new_df = st.session_state.get("pm_basic_uploaded_df")
            raw_df = st.session_state.get("pm_basic_uploaded_raw_df")

            if not spreadsheet_id:
                st.error("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã‚’å…ˆã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
            elif not sheets_service:
                st.error("Sheets API ã®ã‚µãƒ¼ãƒ“ã‚¹ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            elif new_df is None or raw_df is None:
                st.error("å·®åˆ†ãŒè¨ˆç®—ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã«ã€å·®åˆ†ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            else:
                try:
                    # --- ç‰©ä»¶åŸºæœ¬æƒ…å ±ã‚·ãƒ¼ãƒˆã‚’æ–°ã—ã„å†…å®¹ã§å…¨ç½®æ› ---
                    ensure_sheet_and_headers(
                        sheets_service,
                        spreadsheet_id,
                        basic_title,
                        BASIC_COLUMNS,
                    )
                    save_df_to_sheet(
                        sheets_service,
                        spreadsheet_id,
                        basic_title,
                        new_df,
                        BASIC_COLUMNS,
                    )
                    st.success("ç‰©ä»¶åŸºæœ¬æƒ…å ±ã‚·ãƒ¼ãƒˆã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚ï¼ˆæ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã§å…¨è¡Œã‚’ç½®ãæ›ãˆã¦ã„ã¾ã™ï¼‰")

                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸Šã®åŸºæœ¬æƒ…å ±ã‚‚æ›´æ–°
                    basic_df_norm = _normalize_df(new_df, BASIC_COLUMNS)
                    st.session_state["pm_basic_df"] = basic_df_norm

                    # --- ç‰©ä»¶ãƒã‚¹ã‚¿ï¼šæ–°è¦ç®¡ç†ç•ªå·ã ã‘è‡ªå‹•åˆæœŸç™»éŒ² ---
                    ensure_sheet_and_headers(
                        sheets_service,
                        spreadsheet_id,
                        master_title,
                        MASTER_COLUMNS,
                    )
                    current_master_df = load_sheet_as_df(
                        sheets_service,
                        spreadsheet_id,
                        master_title,
                        MASTER_COLUMNS,
                    )

                    candidate_master_df = _map_master_from_raw_df(raw_df)

                    if not candidate_master_df.empty:
                        existing_ids = set(current_master_df["ç®¡ç†ç•ªå·"].astype(str).str.strip())
                        cand_ids = candidate_master_df["ç®¡ç†ç•ªå·"].astype(str).str.strip()
                        mask_new = ~cand_ids.isin(existing_ids)
                        new_master_rows = candidate_master_df[mask_new].copy()

                        if not new_master_rows.empty:
                            updated_master_df = pd.concat(
                                [current_master_df, new_master_rows],
                                ignore_index=True,
                            )
                            save_df_to_sheet(
                                sheets_service,
                                spreadsheet_id,
                                master_title,
                                updated_master_df,
                                MASTER_COLUMNS,
                            )
                            st.session_state["pm_master_df"] = updated_master_df
                            st.success(f"ç‰©ä»¶ãƒã‚¹ã‚¿ã‚·ãƒ¼ãƒˆã«æ–°è¦ {len(new_master_rows)} ä»¶ã‚’è‡ªå‹•ç™»éŒ²ã—ã¾ã—ãŸã€‚")
                        else:
                            updated_master_df = current_master_df
                            st.session_state["pm_master_df"] = updated_master_df
                            st.info("ç‰©ä»¶ãƒã‚¹ã‚¿ã‚·ãƒ¼ãƒˆã«æ–°è¦ç™»éŒ²ã™ã‚‹ç®¡ç†ç•ªå·ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    else:
                        updated_master_df = current_master_df
                        st.session_state["pm_master_df"] = updated_master_df
                        st.info("ç‰©ä»¶ãƒã‚¹ã‚¿ç”¨ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

                    # --- ãƒãƒ¼ã‚¸çµæœã‚‚æ›´æ–°ã—ã¦ãŠã ---
                    merged_df_latest = merge_master_and_basic(updated_master_df, basic_df_norm)
                    st.session_state["pm_merged_df"] = merged_df_latest

                except Exception as e:
                    st.error(f"ç‰©ä»¶åŸºæœ¬æƒ…å ± / ç‰©ä»¶ãƒã‚¹ã‚¿æ›´æ–°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    # ------------------------------
    # ç‰©ä»¶ãƒã‚¹ã‚¿ï¼‹åŸºæœ¬æƒ…å ± èª­ã¿è¾¼ã¿ï¼ˆæ‰‹å‹•ï¼‰
    # ------------------------------
    if load_btn:
        if not spreadsheet_id:
            st.error("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        elif not sheets_service:
            st.error("Sheets API ã®ã‚µãƒ¼ãƒ“ã‚¹ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        else:
            try:
                ensure_sheet_and_headers(
                    sheets_service,
                    spreadsheet_id,
                    basic_title,
                    BASIC_COLUMNS,
                )
                ensure_sheet_and_headers(
                    sheets_service,
                    spreadsheet_id,
                    master_title,
                    MASTER_COLUMNS,
                )

                basic_df = load_sheet_as_df(
                    sheets_service,
                    spreadsheet_id,
                    basic_title,
                    BASIC_COLUMNS,
                )
                master_df = load_sheet_as_df(
                    sheets_service,
                    spreadsheet_id,
                    master_title,
                    MASTER_COLUMNS,
                )

                merged_df = merge_master_and_basic(master_df, basic_df)

                st.session_state["pm_basic_df"] = basic_df
                st.session_state["pm_master_df"] = master_df
                st.session_state["pm_merged_df"] = merged_df
                st.success("ç‰©ä»¶ãƒã‚¹ã‚¿ ï¼‹ åŸºæœ¬æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
            except Exception as e:
                st.error(f"ã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    merged_df: Optional[pd.DataFrame] = st.session_state.get("pm_merged_df")

    if merged_df is None or merged_df.empty:
        st.info("ä¸Šéƒ¨ã®ã€ç‰©ä»¶ãƒã‚¹ã‚¿ ï¼‹ åŸºæœ¬æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€ã€ãƒœã‚¿ãƒ³ã€ã¾ãŸã¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆåæ˜ ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return

    # ------------------------------
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    # ------------------------------
    with st.expander("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            keyword = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆç®¡ç†ç•ªå· / ç‰©ä»¶å / ä½æ‰€ãªã©ï¼‰", key="pm_keyword")
        with col2:
            only_has_master = st.checkbox(
                "ç‰©ä»¶ãƒã‚¹ã‚¿ã«ç™»éŒ²ãŒã‚ã‚‹ç®¡ç†ç•ªå·ã®ã¿è¡¨ç¤º",
                value=False,
                key="pm_only_has_master",
            )

    df_view = merged_df.copy()

    if keyword:
        kw = keyword.strip()
        mask = pd.Series(False, index=df_view.index)
        for col in ["ç®¡ç†ç•ªå·", "ç‰©ä»¶å", "ä½æ‰€", "çª“å£ä¼šç¤¾", "æ‹…å½“éƒ¨ç½²", "æ‹…å½“è€…å"]:
            if col in df_view.columns:
                mask |= df_view[col].astype(str).str.contains(kw, case=False, na=False)
        df_view = df_view[mask]

    if only_has_master:
        master_cols_for_check = [
            "ç‚¹æ¤œå®Ÿæ–½æœˆ",
            "é€£çµ¡æœŸé™_æ—¥å‰",
            "é€£çµ¡æ–¹æ³•_é›»è©±1",
            "é€£çµ¡æ–¹æ³•_é›»è©±2",
            "é€£çµ¡æ–¹æ³•_FAX1",
            "é€£çµ¡æ–¹æ³•_FAX2",
            "é€£çµ¡æ–¹æ³•_ãƒ¡ãƒ¼ãƒ«1",
            "é€£çµ¡æ–¹æ³•_ãƒ¡ãƒ¼ãƒ«2",
        ]
        has_any = pd.Series(False, index=df_view.index)
        for col in master_cols_for_check:
            if col in df_view.columns:
                has_any |= df_view[col].astype(str).str.strip() != ""
        df_view = df_view[has_any]

    # å‰Šé™¤ç”¨ã®ã€Œé¸æŠã€åˆ—è¿½åŠ 
    if "é¸æŠ" not in df_view.columns:
        df_view.insert(0, "é¸æŠ", False)

    st.caption("â€» ç‰©ä»¶åŸºæœ¬æƒ…å ±ã¯ã€ç‰©ä»¶åŸºæœ¬æƒ…å ±ã€ã‚·ãƒ¼ãƒˆã€ç‰©ä»¶ãƒã‚¹ã‚¿ã¯ã€ç‰©ä»¶ãƒã‚¹ã‚¿ã€ã‚·ãƒ¼ãƒˆã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚åŸºæœ¬æƒ…å ±ã‚’ç·¨é›†ã—ãŸã„å ´åˆã¯ã€Excel/CSV ã‚’æ›´æ–°ã—ã¦å†ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ãã ã•ã„ã€‚")

    edited_df = st.data_editor(
        df_view,
        num_rows="dynamic",
        key="pm_editor",
        use_container_width=True,
        hide_index=True,
    )

    col_a, col_b, col_c = st.columns(3)
    with col_a:
        if st.button("é¸æŠè¡Œã‚’å‰Šé™¤"):
            if "é¸æŠ" in edited_df.columns:
                edited_df = edited_df[~edited_df["é¸æŠ"]].copy()
                st.session_state["pm_merged_df"] = edited_df.drop(columns=["é¸æŠ"])
                st.success("é¸æŠã•ã‚ŒãŸè¡Œã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚ï¼ˆä¿å­˜ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨ã€ç‰©ä»¶ãƒã‚¹ã‚¿ã€ã‚·ãƒ¼ãƒˆã«åæ˜ ã•ã‚Œã¾ã™ï¼‰")
            else:
                st.warning("é¸æŠåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    with col_b:
        if st.button("æ–°è¦è¡Œã‚’è¿½åŠ "):
            new_row = {col: "" for col in edited_df.columns}
            new_row["é¸æŠ"] = False
            edited_df = pd.concat([edited_df, pd.DataFrame([new_row])], ignore_index=True)
            st.session_state["pm_merged_df"] = edited_df.drop(columns=["é¸æŠ"])
            st.success("ç©ºã®è¡Œã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚ï¼ˆä¿å­˜ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨ã€ç‰©ä»¶ãƒã‚¹ã‚¿ã€ã‚·ãƒ¼ãƒˆã«åæ˜ ã•ã‚Œã¾ã™ï¼‰")

    with col_c:
        save_btn = st.button("ã€ç‰©ä»¶ãƒã‚¹ã‚¿ã€ã‚·ãƒ¼ãƒˆã«ä¿å­˜", type="primary")

    # ------------------------------
    # ç‰©ä»¶ãƒã‚¹ã‚¿ã‚·ãƒ¼ãƒˆã¸ã®ä¿å­˜
    # ------------------------------
    if save_btn:
        if not spreadsheet_id:
            st.error("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDãŒæœªå…¥åŠ›ã§ã™ã€‚")
            return
        if not sheets_service:
            st.error("Sheets API ã®ã‚µãƒ¼ãƒ“ã‚¹ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return

        save_df = edited_df.drop(columns=["é¸æŠ"]) if "é¸æŠ" in edited_df.columns else edited_df.copy()

        # ç‰©ä»¶ãƒã‚¹ã‚¿ç”¨ã®åˆ—ã ã‘æŠ½å‡º
        master_only = _normalize_df(save_df, MASTER_COLUMNS)

        # æ›´æ–°æ—¥æ™‚ãƒ»æœ€çµ‚æ›´æ–°è€…
        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        if "æ›´æ–°æ—¥æ™‚" in master_only.columns:
            master_only["æ›´æ–°æ—¥æ™‚"] = now_str
        if "æœ€çµ‚æ›´æ–°è€…" in master_only.columns and current_user_email:
            master_only["æœ€çµ‚æ›´æ–°è€…"] = current_user_email

        try:
            ensure_sheet_and_headers(
                sheets_service,
                spreadsheet_id,
                master_title,
                MASTER_COLUMNS,
            )
            save_df_to_sheet(
                sheets_service,
                spreadsheet_id,
                master_title,
                master_only,
                MASTER_COLUMNS,
            )
            st.session_state["pm_master_df"] = master_only

            # æœ€æ–°ã®åŸºæœ¬æƒ…å ±ã¨å†ãƒãƒ¼ã‚¸
            basic_df = st.session_state.get("pm_basic_df") or load_sheet_as_df(
                sheets_service,
                spreadsheet_id,
                basic_title,
                BASIC_COLUMNS,
            )
            merged_df_latest = merge_master_and_basic(master_only, basic_df)
            st.session_state["pm_merged_df"] = merged_df_latest

            st.success("ã€ç‰©ä»¶ãƒã‚¹ã‚¿ã€ã‚·ãƒ¼ãƒˆã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        except Exception:
            # ã‚¨ãƒ©ãƒ¼ã¯ save_df_to_sheet / ensure å†…ã§è¡¨ç¤ºæ¸ˆã¿
            pass
